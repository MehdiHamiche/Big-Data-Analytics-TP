{"cells":[{"cell_type":"markdown","metadata":{"id":"kKcAuYNYGPvq"},"source":["# TP4 Supervised learning: Regression problem (Correction)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jxfbmFQpLv64"},"outputs":[],"source":["pip install scikit-optimize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-pkdaeEGPvp"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"guM7YPnKGPvq"},"source":["### Part 1: Descriptive statistics and preprocessing"]},{"cell_type":"markdown","metadata":{"id":"Be-GuHKs8HHK"},"source":["1) Load _*train.csv*_ and _*test.csv*_ datasets, print their shapes, display the first few rows, and provide a summary with [pandas.DataFrame.describe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) for each dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAKOos8cwLj8","scrolled":true},"outputs":[],"source":["# Answer\n","import pandas as pd\n","train_data = pd.read_csv(\"data/train.csv\")\n","test_data = pd.read_csv(\"data/test.csv\")\n","\n","print(\"train shape: \", train_data.shape)\n","print(\"test shape: \", test_data.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akIfUlpBGPvs"},"outputs":[],"source":["print(\"Train head:\")\n","display(train_data.head(3))\n","print(\"Test head:\")\n","test_data.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z011aS48GPvt"},"outputs":[],"source":["print(\"train dataset description:\")\n","display(train_data.describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OD-4YRNIGPvt"},"outputs":[],"source":["print(\"test dataset description:\")\n","display(test_data.describe())"]},{"cell_type":"markdown","metadata":{"id":"XXlgdqGBGPvu"},"source":["2) Extract ```SalePrice``` as the target variable from the ```train``` and ```test``` datasets, storing them as ```train_target``` and ```test_target``` respectively. Remove unnecessary variables from the same datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"of-cbFS5GPvu"},"outputs":[],"source":["# Answer\n","train_target = train_data[\"SalePrice\"]\n","test_target = test_data[\"SalePrice\"]\n","train_data.drop([\"Id\", \"SalePrice\"], axis = 1, inplace=True)\n","test_data.drop([\"Id\", \"SalePrice\"], axis = 1, inplace=True)\n","\n","# Verify that the ID and SalePrice variable are correctly deleted\n","print(\"Train head:\")\n","display(train_data.head(3))\n","print(\"Test head:\")\n","display(test_data.head(3))"]},{"cell_type":"markdown","metadata":{"id":"XYhQi8V4HOlA"},"source":["3) Define a function that identifies variables with missing values, and returns each variable's name, the number of missing values, and the percentage of missing values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nb7MzIs9GPvu"},"outputs":[],"source":["# Answer\n","def missing_values(data):\n","    missing_values = data.isna().sum().sort_values(ascending=False)\n","    n_missing_values = missing_values[missing_values>0]\n","    p_missing_values = (n_missing_values/data.shape[0])*100\n","    missing_data = pd.concat([n_missing_values,p_missing_values], axis=1, keys=['Count', 'Percentage'])\n","    return missing_data\n","\n","print(\"Missing values of train data:\")\n","display(missing_values(train_data))\n","print(\" \")\n","print(\"Missing values of test data:\")\n","display(missing_values(test_data))"]},{"cell_type":"markdown","metadata":{"id":"9khNgR8m9YcS"},"source":["4) For simplicity, fill the missing values with 0 using [pandas.DataFrame.fillna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html) (modify the same dataset)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F06hYuhP98f1"},"outputs":[],"source":["# Answer\n","train_data.fillna(0.0, inplace=True)\n","test_data.fillna(0.0, inplace=True)\n","\n","# Verify that the missing values are well filled (i.e., there's no more missing values)\n","print(\"Missing values of train data:\")\n","print(missing_values(train_data))\n","print(\" \")\n","print(\"Missing values of test data:\")\n","print(missing_values(test_data))"]},{"cell_type":"markdown","metadata":{"id":"UcA0NMcU-AU6"},"source":["5) Describe the target variable ```train_target```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cyp_FZEj-Ra8"},"outputs":[],"source":["# Answer\n","print(train_target.describe())\n","print(\"Skewness: \", train_target.skew())\n","print(\"Kurtosis: \", train_target.kurtosis())"]},{"cell_type":"markdown","metadata":{"id":"0VGZoDE4-Y-z"},"source":["6) Plot the histogram and density of ```train_target``` (you can use [seaborn.displot](https://seaborn.pydata.org/generated/seaborn.displot.html) module)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_doDG7PL-0Gz"},"outputs":[],"source":["# Answer\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.displot(train_target,bins=30, kde = True, stat=\"density\", color = 'darkblue')\n","plt.title('Histogram of SalePrice')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"c5onn1vI-1na"},"source":["7) Plot histograms of all other variables using [pandas.DataFrame.hist](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RlD-7xyl_DUe"},"outputs":[],"source":["# Answer\n","train_data.hist(figsize=(20, 20), bins=30, layout=(7, 6));"]},{"cell_type":"markdown","metadata":{"id":"-0YSyM3NGPvw"},"source":["8) Compute and plot the correlation matrix between the variables using [seaborn.heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html). Comment the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r9rlLXFCGPvx"},"outputs":[],"source":["# Answer\n","correlation_matrix = pd.concat([train_data,train_target], axis=1).corr()\n","plt.figure(figsize = (10,8))\n","sns.heatmap(correlation_matrix);"]},{"cell_type":"markdown","metadata":{"id":"bUwy-rgKGPvx"},"source":["9) Visualize correlations between ```SalePrice``` and other variables using a [seaborn.barplot](https://seaborn.pydata.org/generated/seaborn.barplot.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7QVbVmxGPvx"},"outputs":[],"source":["# Answer\n","sale_price_correlation = correlation_matrix.loc['SalePrice', :]\n","sale_price_correlation = sale_price_correlation.sort_values(ascending=False)\n","fig = plt.figure(figsize=(12, 8))\n","sns.barplot(x=sale_price_correlation, y=sale_price_correlation.index, orient='h', palette='flare')\n","plt.xlabel('Correlation')\n","plt.title('Correlation of SalePrice with Other Features')\n","plt.show()\n","fig.autofmt_xdate()"]},{"cell_type":"markdown","metadata":{"id":"O4PprbZ8GPvx"},"source":["10) Visualize the scatter plot of the ```SalePrice``` variable as a function of the ```GrLivArea```. Comment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLyOEwKiGPvx"},"outputs":[],"source":["# Answer\n","plt.figure(figsize = (12,8))\n","sns.scatterplot(data = pd.concat([train_data, train_target], axis=1), x = \"GrLivArea\", y = \"SalePrice\");"]},{"cell_type":"markdown","metadata":{"id":"iyy6EHIGGPvx"},"source":["11) Visualize the boxplot of the ```SalePrice``` variable as a function of the ```OverallQual``` using [seaborn.boxplot](https://seaborn.pydata.org/generated/seaborn.boxplot.html). Interpret the boxplot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ItDT24urGPvx"},"outputs":[],"source":["# Answer\n","plt.figure(figsize = (12,8))\n","sns.boxplot(data=pd.concat([train_data, train_target], axis=1), x=\"OverallQual\", y = \"SalePrice\");"]},{"cell_type":"markdown","metadata":{"id":"4YDNWnrtGPvx"},"source":["12) Visualize the empirical distributions of the train and test dataset (for some variables). Comment the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkOFnEcEGPvx"},"outputs":[],"source":["# Answer\n","n_cols = 3\n","fig = plt.figure(figsize=(3*n_cols,6))\n","for j, i in enumerate(train_data.columns[:n_cols]):\n","    plt.subplot(310+j+1)\n","    sns.distplot(train_data[i], label = 'train')\n","    sns.distplot(test_data[i], label = 'test')\n","    plt.legend()\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"UvJy7baBGPvy"},"source":["### Part 2: Train and evaluate models"]},{"cell_type":"markdown","metadata":{"id":"QJvSCL93GPvy"},"source":["13) Split the data into training and validation data using [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRDA9yJFGPvy"},"outputs":[],"source":["# Answer\n","from sklearn.model_selection import train_test_split\n","tr_x, val_x, tr_y, val_y  = train_test_split(train_data, train_target, shuffle=True, random_state=42, test_size=0.20)"]},{"cell_type":"markdown","metadata":{"id":"5k--QSHnGPvy"},"source":["14) Fit a [linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) (we choose the linear regression to learn to predict the target variable) and measure its performance using the [RMSE](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) (RMSE = Root Mean Square Error) and [MAPE](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html) (MAPE = Mean Absolute Percentage Error) as metric. Measure its performance on the test data. Comment the results.\n","\n","**Recall:**\n","\n","$$RMSE(Y,\\hat{Y}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y_i})^2}\\,\\,\\,\\,\\,\\,\\,\\,MAPE(Y,\\hat{Y}) = \\frac{1}{n} \\sum_{i=1}^n \\frac{|Y_i - \\hat{Y_i}|}{Y_i}$$\n","\n","where $Y$ is the true target and  $\\hat{Y}$ is the predicted target  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LT4_747oGPvy"},"outputs":[],"source":["# Answer\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n","\n","model = LinearRegression()\n","model.fit(tr_x, tr_y)\n","val_pred_y = model.predict(val_x)\n","\n","rmse = mean_squared_error(val_y,val_pred_y, squared=False)\n","print(\"RMSE on validation set: \", rmse)\n","test_pred_y = model.predict(test_data)\n","rmse = mean_squared_error(test_target,test_pred_y, squared=False)\n","print(\"RMSE on test set: \", rmse)\n","\n","mape = mean_absolute_percentage_error(val_y,val_pred_y)\n","print(\"MAPE on validation set: \", mape)\n","mape = mean_absolute_percentage_error(test_target,test_pred_y)\n","print(\"MAPE on test set: \", mape)"]},{"cell_type":"markdown","metadata":{"id":"mwm9BTVHGPvy"},"source":["**Objective: Improve the predictions!**"]},{"cell_type":"markdown","metadata":{"id":"yotRGiwdGPvy"},"source":["15) Train the following models and evaluate their performance on validation and test set.\n","* [K-Nearest Neighbors Regressor (KNeighborsRegressor)](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n","* [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n","* [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ST0uyYzwGPvy"},"outputs":[],"source":["# Answer\n","import numpy as np\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.linear_model import  Ridge\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# Complete your answer here\n","\n","# Answer\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n","\n","model = RandomForestRegressor()\n","model.fit(tr_x, tr_y)\n","val_pred_y = model.predict(val_x)\n","\n","rmse = mean_squared_error(val_y,val_pred_y, squared=False)\n","print(\"RMSE on validation set: \", rmse)\n","test_pred_y = model.predict(test_data)\n","rmse = mean_squared_error(test_target,test_pred_y, squared=False)\n","print(\"RMSE on test set: \", rmse)\n","\n","mape = mean_absolute_percentage_error(val_y,val_pred_y)\n","print(\"MAPE on validation set: \", mape)\n","mape = mean_absolute_percentage_error(test_target,test_pred_y)\n","print(\"MAPE on test set: \", mape)"]},{"cell_type":"markdown","metadata":{"id":"QTUvsc-sGPv6"},"source":["16) Define a function that takes in parameter a dictionnary of models and returns the mean and standard deviation of the MAPE on [cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) with three sets. You can consider the following models:\n","\n","* [Support Vector Regressor (SVR)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n","* [Extreme Gradient Boosting Regressor (XGBRegressor)](https://xgboost.readthedocs.io/en/stable/parameter.html)\n","* [Decision tree regressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n","* [K-Nearest Neighbors Regressor (KNeighborsRegressor)](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n","* [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n","* [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n","* [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n","* [Gradient Boosting Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n","* [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qikQz4OCGPv7"},"outputs":[],"source":["# Answer\n","import numpy as np\n","from sklearn.svm import SVR\n","from xgboost import XGBRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso\n","from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import make_scorer\n","\n","def evaluate(models, X_train, y_train, metric):\n","    for model in models:\n","        models[model]['score'] = cross_val_score(models[model]['model'], X_train, y_train, cv=3,scoring=metric)\n","        print(\"{} : {} (+/- {})\".format(models[model]['name'], models[model]['score'].mean(), models[model]['score'].std()**2))\n","\n","MAPE = make_scorer(mean_absolute_percentage_error)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"roxk25OmGPv7"},"outputs":[],"source":["models = {'gbc': {'model': GradientBoostingRegressor(), 'name': 'GradientBoostingRegressor'},\n","          'xgb': {'model': XGBRegressor(), 'name': 'XGBRegressor'},\n","          'rf': {'model': RandomForestRegressor( n_jobs=-1), 'name':'RandomForestRegressor'},\n","          'tree': {'model': DecisionTreeRegressor(), 'name':'DecisionTreeRegressor'},\n","          'svr': {'model': SVR(), 'name': 'SVR'},\n","          'knn': {'model': KNeighborsRegressor(), 'name': 'KNeighborsRegressor'},\n","          'lr': {'model': LinearRegression(), 'name': 'LinearRegression'},\n","          'ridge': {'model': Ridge(), 'name': 'Ridge'},\n","          'lasso': {'model': Lasso(), 'name': 'Lasso'}\n","         }\n","evaluate(models,  train_data, train_target, metric = MAPE)"]},{"cell_type":"markdown","metadata":{"id":"Xo_hEZExGPv8"},"source":["### Part 4: Fine tunning"]},{"cell_type":"markdown","metadata":{"id":"mAoffs0OGPv8"},"source":["17) Choose three best methods and evaluate their performance on the test data by varying some of their parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-7j_3jg7GPv8"},"outputs":[],"source":["# Answer\n","model = GradientBoostingRegressor(n_estimators=200,learning_rate = 0.1, max_depth=3).fit(train_data,train_target)\n","pred_target = model.predict(test_data)\n","print ('GradientBoostingRegressor: {}'.format(mean_absolute_percentage_error(test_target,pred_target)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kk4s6OKjGPv8"},"outputs":[],"source":["# Answer\n","model = GradientBoostingRegressor(n_estimators=150, max_depth=5).fit(train_data,train_target)\n","pred_target = model.predict(test_data)\n","print ('GradientBoostingRegressor: {}'.format(mean_absolute_percentage_error(test_target,pred_target)))\n","\n","model = RandomForestRegressor(n_estimators=200, max_depth=8).fit(train_data,train_target)\n","pred_target = model.predict(test_data)\n","print ('RandomForestRegressor: {}'.format(mean_absolute_percentage_error(test_target,pred_target)))\n","\n","model = XGBRegressor(n_estimators=300 , max_depth =4 ).fit(train_data,train_target)\n","pred_target = model.predict(test_data)\n","print ('XGBRegressor: {}'.format(mean_absolute_percentage_error(test_target,pred_target)))"]},{"cell_type":"markdown","metadata":{"id":"CvXxA53MGPv8"},"source":["18) Perform an automated parameter search using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) on a model of your choice and assess the performance of the best-tuned model on the test dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qSiE3ezGPv8"},"outputs":[],"source":["# Answer\n","from sklearn.model_selection import GridSearchCV\n","\n","model = GradientBoostingRegressor()\n","\n","params = {\n","    'n_estimators' : [100, 300, 400],\n","    'max_depth' : [2, 4, 8],\n","    'learning_rate':[0.1, 0.05, 0.01]}\n","\n","grid = GridSearchCV(model,param_grid=params,cv=3, scoring=MAPE, n_jobs=-1, verbose = 1 )\n","grid.fit(train_data, train_target)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qzpUaKf7GPv9"},"outputs":[],"source":["print( 'Grid search results: {}'.format(grid.best_params_))\n","\n","# Return the best model:\n","best = grid.best_estimator_\n","\n","print ('MAPE of the best model on test data:', mean_absolute_percentage_error(test_target,best.predict(test_data)))\n","print ('RMSE of the best model on test data:', mean_squared_error(test_target,best.predict(test_data), squared=False))"]},{"cell_type":"markdown","metadata":{"id":"RDU8-rPJGPv9"},"source":["19) Perform an automated parameter search using Bayesian optimization with [gp_minimize](https://scikit-optimize.github.io/stable/modules/generated/skopt.gp_minimize.html) and assess the performance of the best-tuned model on the test dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cs0Y8dU-GPv9"},"outputs":[],"source":["# Answer\n","from skopt import gp_minimize\n","from skopt.space import Real, Integer\n","from skopt.utils import use_named_args\n","\n","space  = [Integer(1, 5, name='max_depth'),\n","          Real(10**-5, 10**0, \"log-uniform\", name='eta'),\n","          Real(10**-5, 10**0, \"log-uniform\", name='subsample')\n","          ]\n","\n","model = XGBRegressor(n_estimators = 300)\n","\n","@use_named_args(space)\n","def objective(**params):\n","    model.set_params(**params)\n","\n","    return np.mean(cross_val_score(model, train_data, train_target, cv=4, n_jobs=-1,\n","                                    scoring=MAPE))\n","res_gp = gp_minimize(objective, space, n_calls=50, random_state=42, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrehvKbiGPv9"},"outputs":[],"source":["print(\"Best score = {}\".format(res_gp.fun))\n","print(\"\"\"Best params:\n","- max_depth = {}\n","- eta = {}\n","- subsample = {}\n","\"\"\".format(res_gp.x[0],res_gp.x[1],res_gp.x[2]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cJH0zKkEGPv9"},"outputs":[],"source":["best_model = XGBRegressor(n_estimators=300, max_depth = 4, eta=0.025603767171805257, subsample=0.14637689184566424)\n","best_model.fit(train_data, train_target)\n","pred_target = best_model.predict(test_data)\n","print(\"MAPE on the test dataset: \", mean_absolute_percentage_error(test_target, pred_target))\n","print(\"RMSE on the test dataset: \", mean_squared_error(test_target, pred_target, squared=False))"]},{"cell_type":"markdown","metadata":{"id":"X3XFSSBxGPv9"},"source":["### Part 5: Ensemble modeling"]},{"cell_type":"markdown","metadata":{"id":"OOFPrEfhGPv9"},"source":["19) **Aggregation:** Fit and predict the target using 4 best models. Then, aggregate the results using the mean and median. Evaluate the performances."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bL3Xj6M1GPv-"},"outputs":[],"source":["# Answer\n","\n","models = {'GradientBoosting': {'model': GradientBoostingRegressor(n_estimators= 300), 'name': 'GradientBoostingRegressor'},\n","          'XGBoost': {'model': XGBRegressor(n_estimators=300), 'name': 'XGBRegressor'},\n","          'RandomForest': {'model': RandomForestRegressor(n_estimators=300, n_jobs=-1), 'name':'RandomForestRegressor'},\n","          'Ridge': {'model': Ridge(), 'name': 'Ridge'}\n","         }\n","predictions = {}\n","for model_name in models:\n","    model = models[model_name][\"model\"].fit(train_data, train_target)\n","    predictions[model_name] = model.predict(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RbzQB4q3GPv-"},"outputs":[],"source":["print ('''Aggregation by mean\n","RMSE = {}\n","MAPE = {}\n","'''.format(mean_squared_error(test_target,np.mean(list(predictions.values()),axis=0),squared=False),mean_absolute_percentage_error(test_target,np.mean(list(predictions.values()),axis=0))))\n","\n","print ('''Aggregation by median\n","RMSE = {}\n","MAPE = {}\n","'''.format(mean_squared_error(test_target,np.median(list(predictions.values()),axis=0),squared=False),mean_absolute_percentage_error(test_target,np.median(list(predictions.values()),axis=0))))\n"]},{"cell_type":"markdown","metadata":{"id":"ASAVI7AXGPv-"},"source":["20) **Stacking:** Perform the following steps:<br>\n","\n","   1. Fit the 4 best models on the ```tr_x``` and save the predictions on ```val_x``` on a new dataframe named ```design_layer1``` and the predictions on ```data_test``` on ```test_layer1```. <br>\n","   2. Fit a new model on the ```design_layer1```. <br>\n","   3. Predict the target using the new model.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-ZuaZ2QGPv-"},"outputs":[],"source":["# Answer\n","models = {'GradientBoosting': {'model': GradientBoostingRegressor(n_estimators= 300), 'name': 'GradientBoostingRegressor'},\n","          'XGBoost': {'model': XGBRegressor(n_estimators=300), 'name': 'XGBRegressor'},\n","          'RandomForest': {'model': RandomForestRegressor(n_estimators=300), 'name':'RandomForestRegressor'},\n","          'Ridge': {'model': Ridge(), 'name': 'Ridge'}\n","         }\n","test_layer1 = pd.DataFrame()\n","design_layer1 = pd.DataFrame()\n","for model_name in models:\n","    model_layer1 = models[model_name][\"model\"].fit(tr_x, tr_y)\n","    design_layer1[model_name] = model_layer1.predict(val_x)\n","    test_layer1[model_name] = model_layer1.predict(test_data)\n","display(design_layer1.head())\n","display(test_layer1.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yrMoFZ6bGPv-"},"outputs":[],"source":["modele_layer2 = LinearRegression()\n","modele_layer2.fit(design_layer1, val_y)\n","pred_target = modele_layer2.predict(test_layer1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APTiqizXGPv-"},"outputs":[],"source":["print('RMSE = ', mean_squared_error(test_target,pred_target, squared=False))\n","print('MAPE = ', mean_absolute_percentage_error(test_target,pred_target))"]}],"metadata":{"accelerator":"GPU","celltoolbar":"Diaporama","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}